{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html, json, re\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./slang_dict.json\",'r') as f:\n",
    "    slang_dict = json.load(f)\n",
    "    \n",
    "with open(\"./emoji_dic.json\",'r') as f:\n",
    "    emoji_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # converting html codes \n",
    "    decoded_text = html.unescape(text)\n",
    "    decoded_text = decoded_text.lower()\n",
    "    # match strings starting with 'http'\n",
    "    text = re.sub(r'http\\S+', '', decoded_text)\n",
    "    # match strings start with '<' and end with '>'\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove emoji\n",
    "    for em, meaning in emoji_dict.items():\n",
    "        text = text.replace(em, meaning)\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    #standard preprocessing technique\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove punctuation\n",
    "    punctuation_removed = [word for word in tokens if word not in list(string.punctuation)]\n",
    "    # lemmatization\n",
    "    lemmatized_text = [WordNetLemmatizer().lemmatize(word) for word in punctuation_removed]\n",
    "    text =  ' '.join(lemmatized_text)\n",
    "\n",
    "    ## removing slangs\n",
    "    words = text.split()\n",
    "    corrected_words = [slang_dict.get(word, word) for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    return text \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True).to(device)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(comment):\n",
    "    inputs = bert_tokenizer.encode_plus(\n",
    "        comment,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Move input tensors to the GPU\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "    # Extract the [CLS] token's embedding and move it back to the CPU\n",
    "    cls_embedding = outputs['last_hidden_state'][:, 0, :].squeeze().cpu().numpy()\n",
    "    return cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = load('svm_classifier_bert.joblib')\n",
    "def infer(X):\n",
    "   X = X.reshape(1, -1)\n",
    "   return loaded_model.predict(X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = \"\"\"Where do I even begin with the Oppenheimer movie? It's a perplexing mess of a film that fails to capture the essence of its subject matter and leaves the audience scratching their heads in confusion. With high expectations due to its talented cast and promising premise, this movie ultimately disappoints on every level.\n",
    "\n",
    "First and foremost, the pacing is an absolute nightmare. The movie meanders aimlessly, dragging out scenes that add little to the plot and leaving essential elements underdeveloped. It's almost as if the filmmakers had no idea how to structure the narrative or maintain a cohesive flow. As a result, the movie feels like a jumbled collection of disconnected events that leave viewers struggling to make sense of the story.\n",
    "\n",
    "The characters in Oppenheimer are equally underwhelming. Despite the exceptional actors involved, their performances are hampered by a lack of depth and poorly written dialogues. The titular character, J. Robert Oppenheimer, comes across as one-dimensional and devoid of real personality or emotional resonance. Supporting characters receive even less attention, leaving us indifferent to their fates and unable to invest in their arcs.\n",
    "\n",
    "The film's attempts at historical accuracy are laughable at best. While some creative liberties are expected in any biographical movie, Oppenheimer takes it to an extreme. The inaccuracies and distortions of actual events not only disrespect the legacy of those involved but also undermine the film's credibility. The filmmakers were more interested in sensationalism than telling a compelling and fact-based story.\n",
    "\n",
    "Perhaps the most egregious aspect of the Oppenheimer movie is its lack of a coherent message or thematic depth. It raises significant moral and ethical questions about the development of nuclear weapons and their consequences, but it never delves into these issues with any real substance. Instead, the movie superficially glazes over these crucial aspects, leaving viewers with a sense of emptiness and missed opportunities.\n",
    "\n",
    "The cinematography and direction do little to salvage the film's shortcomings. The visual style lacks creativity, and the director seems to rely on tired and overused cinematic clich√©s. The lack of a distinct visual identity only adds to the overall mediocrity of the movie.\n",
    "\n",
    "In conclusion, the Oppenheimer movie is a colossal disappointment. Its weak storytelling, poorly developed characters, historical inaccuracies, and lack of a compelling message all contribute to a film that is an absolute failure. Save your time and money and skip this cinematic disaster. There are far better biographical dramas out there that do justice to their subjects and deliver a more engaging and coherent experience. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentiment(review):\n",
    "    processed = preprocess(review)\n",
    "    features = get_bert_embedding(processed)\n",
    "    output = infer(features)\n",
    "    return output\n",
    "\n",
    "get_sentiment(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ekyus\\AppData\\Local\\Temp\\ipykernel_22796\\2782166149.py\", line 14, in on_button_press\n",
      "    sentiment = get_sentiment(review)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ekyus\\AppData\\Local\\Temp\\ipykernel_22796\\2782166149.py\", line 8, in get_sentiment\n",
      "    output = infer(features)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ekyus\\AppData\\Local\\Temp\\ipykernel_22796\\1538846586.py\", line 6, in infer\n",
      "    return loaded_model.predict(X)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 818, in predict\n",
      "    y = super().predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 431, in predict\n",
      "    X = self._validate_for_predict(X)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 611, in _validate_for_predict\n",
      "    X = self._validate_data(\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 604, in _validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 940, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Expected 2D array, got 1D array instead:\n",
      "array=[ 5.47385328e-02  2.61417598e-01  6.54152110e-02 -2.70436645e-01\n",
      " -1.49537817e-01 -4.47605997e-01  5.80349825e-02  3.65289479e-01\n",
      "  1.60262257e-01 -2.79682517e-01  6.39793575e-02 -6.49251565e-02\n",
      " -1.04376730e-02  2.93350637e-01  7.34181404e-02 -5.77428639e-02\n",
      " -1.57252446e-01  2.45360076e-01  7.34303519e-02 -8.90577361e-02\n",
      " -2.16387007e-02 -1.57098725e-01 -1.39902130e-01 -8.63180384e-02\n",
      "  4.58105020e-02 -1.58921868e-01  4.64375019e-02 -1.27840951e-01\n",
      "  1.33228645e-01 -3.20243984e-02  3.61019254e-01  6.50257915e-02\n",
      " -1.41242713e-01  9.17418674e-02 -1.36115095e-02 -3.74798328e-02\n",
      "  2.82309726e-02 -8.16805065e-02 -5.81078753e-02  3.24669257e-02\n",
      " -1.30849093e-01 -6.29314557e-02 -4.53037694e-02  1.49661750e-01\n",
      "  7.81616196e-02 -2.29096308e-01 -1.65784895e+00 -5.21818586e-02\n",
      " -9.01167989e-02 -2.94929475e-01  2.81618118e-01  4.38475162e-02\n",
      "  1.87587097e-01  8.31142440e-02  1.71391323e-01  5.06058289e-03\n",
      " -2.24750698e-01  5.39290309e-01 -7.18460232e-02  3.99437249e-02\n",
      "  3.77347805e-02  7.85251614e-03  2.29199082e-02  1.54987812e-01\n",
      "  1.66680478e-02 -8.72434117e-03 -8.73183385e-02  1.77726790e-01\n",
      " -1.36441933e-02  3.67619067e-01 -2.40825862e-01 -1.00783110e-01\n",
      "  1.23901464e-01 -1.96960047e-01  8.13929886e-02 -1.50817689e-02\n",
      "  1.15109064e-01  1.57911167e-01 -1.67416513e-01 -1.01190053e-01\n",
      " -1.67939588e-02  3.22306812e-01  9.30753723e-02  3.43067981e-02\n",
      "  1.29222170e-01  1.03216916e-01 -3.14524770e-01 -4.61970419e-02\n",
      "  3.42165679e-01  1.20816603e-01 -2.65911907e-01  3.12042445e-01\n",
      "  2.19978970e-02  1.71197921e-01  2.46510863e-01 -8.73431638e-02\n",
      " -2.04371586e-02 -3.51696908e-02  2.48906001e-01  1.42206982e-01\n",
      "  1.31108046e-01 -6.12502918e-02  5.00294976e-02 -7.39266723e-02\n",
      "  9.46643800e-02 -1.22539543e-01  1.12400008e-02 -1.70007832e-02\n",
      "  1.02193914e-01 -3.18775678e+00  2.36241873e-02  1.66560024e-01\n",
      " -1.20186433e-01 -1.50611743e-01 -1.09986685e-01  4.64194179e-01\n",
      "  2.41893008e-01  6.29986674e-02  9.37109143e-02 -1.09672919e-01\n",
      " -1.75871164e-01  4.27600056e-01 -2.07339510e-01  9.08419490e-02\n",
      "  7.42798448e-02  1.97921783e-01  2.23400503e-01 -1.02404699e-01\n",
      "  5.58834858e-02  2.31554080e-02  1.78286627e-01  2.26599142e-01\n",
      " -3.80764902e-02 -6.95900843e-02 -2.37329572e-01  7.71920532e-02\n",
      "  6.03576079e-02 -5.24626859e-02 -5.72263412e-02 -1.31929338e-01\n",
      " -1.02216944e-01 -1.41120508e-01 -3.22147703e+00  3.07401359e-01\n",
      "  6.72980919e-02  9.50497314e-02  2.22394522e-02 -8.01568031e-02\n",
      "  2.34314781e-02  2.98169125e-02  9.12136398e-04 -2.27594208e-02\n",
      " -2.63638616e-01  1.40143573e-01 -3.19485031e-02 -8.85985140e-03\n",
      " -7.77735338e-02 -2.52192438e-01  4.36203182e-01  2.71816075e-01\n",
      "  1.74570113e-01 -1.10474579e-01  6.80771843e-02 -2.24237710e-01\n",
      "  1.67327859e-02 -1.21087648e-01  3.71846706e-02  1.54857263e-01\n",
      "  2.34521538e-01 -4.65153158e-02 -4.23577279e-02  3.00693691e-01\n",
      " -1.11832260e-03  3.34460922e-02  1.29827857e-01 -2.24387988e-01\n",
      " -2.74714176e-02  2.13152096e-01  5.48371300e-02  1.93837777e-01\n",
      " -1.16187833e-01  2.53739566e-01  7.31053352e-02  2.75240131e-02\n",
      "  1.27237007e-01  5.09905964e-02  2.30229706e-01 -1.01258397e-01\n",
      " -1.65877134e-01  3.30020636e-01 -1.69811808e-02 -5.67349195e-02\n",
      "  3.15803625e-02  8.80359113e-02  2.81283975e-01 -3.33910398e-02\n",
      "  3.21014374e-02 -2.90212572e-01  1.91798836e-01  3.30932975e-01\n",
      " -9.25233439e-02 -1.68149471e-01 -1.57089084e-01  1.60064548e-01\n",
      " -1.15367547e-02  3.84895325e+00  1.64203122e-01 -7.19838068e-02\n",
      "  2.52359599e-01  5.04855104e-02 -5.03167845e-02 -2.68394556e-02\n",
      " -1.74801961e-01 -1.24230348e-01  4.60052229e-02 -1.59421563e-01\n",
      " -4.66888014e-04  8.38009715e-02 -4.47385907e-02  1.30561978e-01\n",
      "  2.44040087e-01  3.81759524e-01 -5.04083820e-02  2.11759672e-01\n",
      "  6.48287609e-02 -1.21551849e-01 -6.69555664e-02  2.75520176e-01\n",
      " -4.16195132e-02 -1.03213894e+00 -2.96847802e-02  7.82339424e-02\n",
      " -3.78094465e-01  3.47744465e-01 -1.08785845e-01  1.29798092e-02\n",
      "  1.40623242e-01 -8.34171399e-02 -4.91780601e-02  6.67393208e-02\n",
      "  1.04127228e-01  6.25893995e-02 -3.34562212e-02  2.65400678e-01\n",
      " -1.51671097e-01  1.81771398e-01  2.18398482e-01 -5.01356088e-02\n",
      " -5.52146463e-03  1.60028845e-01  1.45663217e-01  6.43246397e-02\n",
      "  1.40705556e-01 -1.74014375e-01  9.38281268e-02  1.21362463e-01\n",
      "  1.50051624e-01  3.40598300e-02 -1.97289214e-01 -2.22433254e-01\n",
      " -1.26762003e-01  1.58407584e-01 -1.21613108e-01  1.10040501e-01\n",
      " -1.91810250e-01 -1.47546098e-01  1.02324419e-01  9.16629806e-02\n",
      "  1.48538888e-01 -7.68534765e-02 -5.93353361e-02  1.85442604e-02\n",
      " -1.46394819e-01 -5.08710003e+00 -5.08935228e-02  1.40772924e-01\n",
      "  2.75214881e-01  2.83827543e-01 -6.44121021e-02  2.23731801e-01\n",
      "  1.36957020e-01  1.26436666e-01 -3.29970866e-01  2.47510850e-01\n",
      "  1.05081663e-01 -1.25709638e-01  3.13168287e-01 -2.41610035e-01\n",
      " -4.23174836e-02  4.82138246e-02 -2.37051304e-02 -2.51177877e-01\n",
      " -7.77180493e-02  2.50454545e-01  2.92110503e-01 -3.18441540e-01\n",
      "  6.27669469e-02 -7.87311643e-02 -1.20822601e-01 -2.14362130e-01\n",
      " -2.74398662e-02  3.35051060e-01 -3.44721451e-02  1.62676543e-01\n",
      " -2.24185690e-01  7.55289495e-02  9.72879827e-02  3.17483172e-02\n",
      " -2.07966852e+00  7.90346563e-02 -3.21386307e-01 -1.64574310e-02\n",
      "  1.32954065e-02  9.64969210e-03  9.16894451e-02 -4.19542864e-02\n",
      " -1.34517983e-01  1.78487062e-01 -7.25933313e-02  1.18908003e-01\n",
      " -9.92027223e-02  3.03712320e-02  1.49239302e-01  8.41841195e-03\n",
      "  9.05367807e-02  1.47605821e-01  1.42044192e-02  7.92967975e-02\n",
      " -9.20116603e-02  2.72104710e-01 -1.23545296e-01  1.50404409e-01\n",
      "  7.93828294e-02  1.50779322e-01 -1.34313598e-01 -3.65540236e-02\n",
      " -2.10611429e-02 -2.52037682e-02  1.01411439e-01 -3.06795202e-02\n",
      "  9.88909602e-02 -1.21564344e-01 -1.78911403e-01 -7.53503367e-02\n",
      "  1.39391646e-01  9.39814523e-02  3.23449820e-01  1.24598935e-01\n",
      " -1.00299686e-01  4.00568426e-01  1.00588173e-01  4.02465612e-01\n",
      "  2.35861734e-01  1.10283203e-01 -4.67461646e-02 -2.13783383e-01\n",
      " -1.33364782e-01 -4.52089421e-02  8.63261521e-02 -6.77184761e-02\n",
      "  1.08312702e+00 -4.97113541e-03 -6.32630021e-04 -8.19945410e-02\n",
      "  1.73140585e-01  7.28278980e-02  2.07569957e-01  4.33169082e-02\n",
      "  3.32557708e-01 -9.73907784e-02  2.58933514e-01  1.20624164e-02\n",
      "  1.52470395e-02 -2.00546950e-01  2.48304665e-01 -2.75590062e-01\n",
      "  1.88575909e-01 -4.98774054e-04  1.13928452e-01 -3.58741246e-02\n",
      "  2.48296317e-02 -5.83861172e-01 -9.69965085e-02  2.50378121e-02\n",
      " -2.14575052e-01  2.86116689e-01  2.11632505e-01  3.48828919e-02\n",
      " -9.09212977e-02 -1.35181084e-01  7.56635070e-02  9.12933797e-02\n",
      " -9.56228524e-02 -1.68732330e-01  1.31473556e-01 -2.79242862e-02\n",
      "  7.84952864e-02  9.31288525e-02  2.87282597e-02  2.31547669e-01\n",
      " -1.17451392e-01 -1.09206103e-01  1.39698088e-01  7.28157014e-02\n",
      "  3.33143137e-02 -7.75241852e-01 -5.69660030e-03 -3.48869190e-02\n",
      " -2.41869897e-01 -2.09285304e-01 -2.21085802e-01  1.51033640e-01\n",
      " -5.27529418e-02 -2.06383035e-01  1.79064885e-01  2.72903621e-01\n",
      " -9.72337574e-02  1.57078966e-01  5.48191406e-02  3.82722244e-02\n",
      "  1.75872073e-01  9.73094776e-02  5.82592249e-01 -1.75650045e-02\n",
      "  5.20936064e-02  2.23314136e-01 -9.95770246e-02  1.23885563e-02\n",
      "  2.86608994e-01  3.33474725e-02 -5.28653413e-02  3.47997658e-02\n",
      " -3.08849752e-01 -6.30164668e-02 -8.83681551e-02 -3.71652544e-01\n",
      " -2.01514006e-01 -1.11363873e-01  1.44018233e-01  2.69413530e-03\n",
      " -2.87591368e-01 -4.85481530e-01 -9.01562870e-02 -6.48665577e-02\n",
      " -1.87544629e-01  5.93186505e-02 -1.21355459e-01  1.81980759e-01\n",
      "  2.45989729e-02  6.19079657e-02 -1.35158494e-01  1.80465832e-01\n",
      " -1.50122643e-01  2.04576910e-01 -9.96359363e-02 -2.68694647e-02\n",
      " -4.72595133e-02  2.61921227e-01  1.89321693e-02 -1.98546797e-01\n",
      "  1.46446466e-01 -1.05103992e-01 -3.37947346e-02  1.94226474e-01\n",
      "  2.03952596e-01 -3.64615358e-02  1.25421822e-01  2.72939056e-01\n",
      "  8.89311312e-04  6.39728308e-02 -1.04296625e+00  1.91505656e-01\n",
      "  2.60751635e-01  1.58083752e-01 -2.92004999e-02 -9.18093324e-02\n",
      " -1.86872840e-01  4.47815210e-01 -3.42490338e-02  4.10050191e-02\n",
      " -2.08733350e-01  1.20953478e-01  5.48826307e-02 -1.81560874e-01\n",
      " -1.32058218e-01  1.68754697e-01  2.74095118e-01 -1.73241898e-01\n",
      " -7.02458397e-02  1.39048707e-03  1.32251182e-03  3.73878062e-01\n",
      " -2.00063765e-01 -1.47452831e-01 -1.76259018e-02  2.10594758e-02\n",
      "  7.85348471e-03  2.57608026e-01 -1.46242931e-01  3.83031219e-02\n",
      "  2.10609093e-01 -1.79477483e-01 -3.59855801e-01  5.08104786e-02\n",
      " -1.46802366e-01 -1.37118667e-01  1.51435718e-01  3.67188714e-02\n",
      "  3.15217823e-01  2.75133908e-01 -1.63716108e-01  3.76527160e-01\n",
      "  8.88974443e-02 -2.26593107e-01  3.69582474e-01  2.49735609e-01\n",
      " -1.83655709e-01  2.50319958e-01  1.29577070e-01 -1.77291512e-01\n",
      " -5.98127693e-02 -1.26950070e-01 -6.86386898e-02 -1.41357049e-01\n",
      "  1.23996794e-01 -9.55541655e-02  7.64490664e-02  6.97368011e-02\n",
      "  6.42576665e-02 -2.30359118e-02  2.14912474e-01 -3.46581116e-02\n",
      " -9.13935676e-02  1.54518083e-01 -1.91547170e-01 -3.24729949e-01\n",
      " -1.31192952e-01 -2.64432374e-02 -1.22857668e-01 -3.94111834e-02\n",
      "  2.40010589e-01 -5.89112490e-02 -4.23122309e-02  1.36396930e-01\n",
      " -1.96192592e-01  2.92617083e-01  2.06893876e-01  1.70459166e-01\n",
      "  1.86561435e-01 -1.32657647e-01 -1.01952955e-01 -7.99682140e-02\n",
      " -7.72429407e-02 -8.01268891e-02  9.31945741e-02  2.03628957e-01\n",
      " -8.21429789e-02 -1.81138646e-02 -1.86320812e-01 -1.79723367e-01\n",
      " -2.21282095e-01 -1.46870434e-01  4.11439538e-01 -5.03211655e-02\n",
      " -6.02763332e-02  1.00582793e-01  2.54009783e-01  3.82861532e-02\n",
      "  5.34056164e-02  4.55812104e-02 -2.83862442e-01  3.14866573e-01\n",
      " -4.19922695e-02  3.79666612e-02  2.75694616e-02  1.05480723e-01\n",
      "  3.13126564e-01  2.83069327e-04 -1.74549952e-01 -1.37735754e-01\n",
      " -9.16422084e-02  3.11131537e-01 -1.94214866e-01 -1.70960501e-01\n",
      " -1.25250548e-01 -5.03381640e-02  3.49464476e-01 -1.76737532e-01\n",
      "  2.72681093e+00  2.26357102e-01  2.36858517e-01 -1.61085337e-01\n",
      "  1.64353792e-02  3.20697241e-02  7.11643696e-02  1.16565842e-02\n",
      " -1.97621077e-01  7.07496554e-02 -2.67899465e-02 -9.55699831e-02\n",
      "  1.32129386e-01  7.92166069e-02  7.17739463e-02 -3.36505733e-02\n",
      " -1.64560489e-02 -9.77760255e-02 -3.02390069e-01 -1.80769965e-01\n",
      " -1.62827075e-01  2.27197394e-01  3.31470221e-02 -2.75157392e-01\n",
      "  1.62637591e-01  2.84899235e-01 -6.38129041e-02  1.78517938e-01\n",
      " -7.77901113e-02 -5.16259409e-02  8.79588053e-02 -1.48725837e-01\n",
      "  9.25137624e-02  3.28567624e-02 -1.44220129e-01  3.13950062e-01\n",
      "  2.85580475e-02 -4.48213629e-02  2.93902546e-01 -2.03001335e-01\n",
      " -8.61725807e-02 -1.05771393e-01  5.94809502e-02 -1.62581876e-01\n",
      "  5.65527603e-02  6.89811260e-03  1.00182354e-01  3.33332978e-02\n",
      "  5.69543764e-02  8.63606185e-02 -2.62789369e-01 -1.27946317e-01\n",
      " -1.62196219e-01  1.61502123e-01 -1.25573725e-01 -7.13465884e-02\n",
      "  2.79711932e-01 -1.63111806e-01 -1.84677988e-01  2.25831956e-01\n",
      " -8.52860287e-02 -1.02926478e-01  2.53362626e-01  4.85061929e-02\n",
      " -3.57112451e-03  4.87860888e-02  2.89309621e-02 -9.06576496e-03\n",
      "  2.66196746e-02 -8.44783634e-02 -4.10002396e-02  1.55794933e-01\n",
      "  8.54551867e-02 -1.34691536e-01  7.55925849e-02 -4.46326025e-02\n",
      "  2.99493462e-01  8.78267363e-02 -1.06120951e-01 -3.77624702e+00\n",
      "  1.16909713e-01 -5.23295552e-02 -3.65599170e-02  4.98193986e-05\n",
      "  3.06976199e-01  2.44637057e-01  9.65700373e-02 -6.01629615e-02\n",
      " -2.31451876e-02  2.18185782e-01  1.36002466e-01  1.63468018e-01\n",
      "  1.33130223e-01  1.30559772e-01  1.60409305e-02  3.57120782e-02\n",
      " -1.14517517e-01 -1.64609458e-02 -1.02261640e-01  1.05957061e-01\n",
      "  2.05646846e-02 -6.86785504e-02  4.45547253e-02 -7.76268691e-02\n",
      "  9.14202109e-02 -2.72997823e-02 -2.10306510e-01  1.64640322e-01\n",
      "  1.34994075e-01  2.26361752e-02  1.76166162e-01  9.39958554e-04\n",
      "  1.06649548e-01 -1.60870567e-01 -5.96029162e-02 -9.03527364e-02\n",
      "  1.07689232e-01  1.09952845e-01  1.68927550e-01 -5.39232371e-03\n",
      "  2.28699490e-01  4.79083173e-02 -1.59194261e-01 -1.48744673e-01\n",
      " -1.09327368e-01  3.51609200e-01 -1.16591811e-01  1.22547284e-01\n",
      " -1.23614311e-01 -2.53141373e-02  1.35504588e-01 -9.26683620e-02\n",
      " -3.41943614e-02  1.55571818e-01 -4.88263890e-02  8.28656778e-02\n",
      "  1.82641014e-01  1.76528282e-02 -1.44613549e-01  1.54755220e-01\n",
      " -3.13437395e-02  4.31382023e-02 -1.45907521e-01  2.83584565e-01\n",
      " -4.04605538e-01  6.55476898e-02  5.50146699e-02 -2.17417598e-01\n",
      "  6.35607839e-02  2.43410957e-03  1.01226367e-01  1.89069629e-01\n",
      " -1.23400308e-01  2.22880110e-01  5.04689179e-02  2.80930430e-01\n",
      "  4.28776264e-01  6.88042492e-02  2.42659505e-02 -2.84363385e-02\n",
      " -2.78855464e-03 -1.29265890e-01 -1.80162583e-03 -7.03741014e-02\n",
      " -9.41939163e+00 -7.30013475e-02 -1.50661878e-02 -1.47999808e-01\n",
      "  3.18028517e-02  7.23850504e-02 -1.04672588e-01 -1.41814798e-01\n",
      "  4.92331237e-02  6.47955239e-02  4.99822991e-03  2.23869272e-02\n",
      " -6.94800615e-02  3.47837023e-02  7.27424920e-02  1.31416976e-01].\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Define the function that will determine the sentiment.\n",
    "def get_sentiment(review):\n",
    "    processed = preprocess(review)\n",
    "    features = get_bert_embedding(processed)\n",
    "    output = infer(features)\n",
    "    return output\n",
    "\n",
    "# Function to be called when the \"Get Sentiment\" button is pressed.\n",
    "def on_button_press():\n",
    "    review = entry.get(\"1.0\", \"end-1c\")  # Get text from the entry widget.\n",
    "    sentiment = get_sentiment(review)\n",
    "    messagebox.showinfo(\"Sentiment Result\", f\"The sentiment is: {sentiment}\")\n",
    "\n",
    "# Create the main window.\n",
    "root = tk.Tk()\n",
    "root.title(\"Sentiment Analysis\")\n",
    "\n",
    "# Create and pack widgets.\n",
    "label = tk.Label(root, text=\"Enter your review:\")\n",
    "label.pack(pady=20)\n",
    "\n",
    "entry = tk.Text(root, height=10, width=50)\n",
    "entry.pack(pady=20)\n",
    "\n",
    "button = tk.Button(root, text=\"Get Sentiment\", command=on_button_press)\n",
    "button.pack(pady=20)\n",
    "\n",
    "# Run the application.\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Comment</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edward Spence</td>\n",
       "      <td>justin trudeau is really desperate pierre poli...</td>\n",
       "      <td>2 stars</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Edward Spence</td>\n",
       "      <td>just trudeau will loose the next election pier...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cedric Farrell</td>\n",
       "      <td>thanks you for your service mr trudeau. god bl...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wapn Perfo</td>\n",
       "      <td>every person here should be ashamed of themsel...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marcel Dagenais</td>\n",
       "      <td>so why did justin not complete his tenure as a...</td>\n",
       "      <td>2 stars</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>Jean-Guy Rubberboot</td>\n",
       "      <td>inconceivable that people still voted for this...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>Cookie Cute as a puppy</td>\n",
       "      <td>:clapping_hands_medium_skin_tone::clapping_han...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>Oscar Vandermeer</td>\n",
       "      <td>our little dictator, his father gave us the ch...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>Steve-o Moreno</td>\n",
       "      <td>if you wanna leave canada, you guys are welcom...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>Nick Glaws</td>\n",
       "      <td>this loser again. he will try to force everybo...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3880 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Author   \n",
       "0              Edward Spence  \\\n",
       "1              Edward Spence   \n",
       "2             Cedric Farrell   \n",
       "3                 Wapn Perfo   \n",
       "4            Marcel Dagenais   \n",
       "...                      ...   \n",
       "3876     Jean-Guy Rubberboot   \n",
       "3877  Cookie Cute as a puppy   \n",
       "3878        Oscar Vandermeer   \n",
       "3879          Steve-o Moreno   \n",
       "3880              Nick Glaws   \n",
       "\n",
       "                                                Comment sentiment_label   \n",
       "0     justin trudeau is really desperate pierre poli...         2 stars  \\\n",
       "1     just trudeau will loose the next election pier...          1 star   \n",
       "2     thanks you for your service mr trudeau. god bl...         5 stars   \n",
       "3     every person here should be ashamed of themsel...          1 star   \n",
       "4     so why did justin not complete his tenure as a...         2 stars   \n",
       "...                                                 ...             ...   \n",
       "3876  inconceivable that people still voted for this...          1 star   \n",
       "3877  :clapping_hands_medium_skin_tone::clapping_han...          1 star   \n",
       "3878  our little dictator, his father gave us the ch...         5 stars   \n",
       "3879  if you wanna leave canada, you guys are welcom...         5 stars   \n",
       "3880  this loser again. he will try to force everybo...          1 star   \n",
       "\n",
       "        labels  \n",
       "0     negative  \n",
       "1     negative  \n",
       "2     positive  \n",
       "3     negative  \n",
       "4     negative  \n",
       "...        ...  \n",
       "3876  negative  \n",
       "3877  negative  \n",
       "3878  positive  \n",
       "3879  positive  \n",
       "3880  negative  \n",
       "\n",
       "[3880 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use python emoji library to deal with emoji\n",
    "# reference: https://carpedm20.github.io/emoji/docs/index.html\n",
    "# the converted text is sth like :smiley_face:\n",
    "# need to remove ':' and replace '_' with ' '\n",
    "\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "df['Comment'] = df['Comment'].apply(convert_emojis)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_cleaned_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply basic preprocessing to the Comment column\n",
    "df['processed_comment'] = df['Comment'].apply(basic_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets (80-20 split)\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=11, stratify=df['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['processed_comment'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['processed_comment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(comment):\n",
    "    words = comment.split()\n",
    "    embeddings = [glove_model[word] for word in words if word in glove_model.key_to_index]\n",
    "    \n",
    "    if not embeddings:\n",
    "        return np.zeros(glove_model.vector_size)\n",
    "    \n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "X_train_glove = np.array([get_glove_embedding(comment) for comment in train_data['processed_comment']])\n",
    "X_test_glove = np.array([get_glove_embedding(comment) for comment in test_data['processed_comment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True).to(device)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embedding(comment):\n",
    "    inputs = bert_tokenizer.encode_plus(\n",
    "        comment,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Move input tensors to the GPU\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "    # Extract the [CLS] token's embedding and move it back to the CPU\n",
    "    cls_embedding = outputs['last_hidden_state'][:, 0, :].squeeze().cpu().numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "\n",
    "X_train_bert = np.array([get_bert_embedding(comment) for comment in train_data['processed_comment']])\n",
    "X_test_bert = np.array([get_bert_embedding(comment) for comment in test_data['processed_comment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train the classifier using the training data (TF-IDF representations)\n",
    "svm_classifier.fit(X_train_tfidf, train_data['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8810096153846154\n",
      "Testing Accuracy: 0.7451923076923077\n",
      "\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.99      0.92      1745\n",
      "     neutral       1.00      0.18      0.30       151\n",
      "    positive       0.96      0.73      0.83       600\n",
      "\n",
      "    accuracy                           0.88      2496\n",
      "   macro avg       0.94      0.63      0.69      2496\n",
      "weighted avg       0.89      0.88      0.86      2496\n",
      "\n",
      "\n",
      "Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.96      0.84       436\n",
      "     neutral       0.00      0.00      0.00        38\n",
      "    positive       0.70      0.32      0.44       150\n",
      "\n",
      "    accuracy                           0.75       624\n",
      "   macro avg       0.48      0.43      0.43       624\n",
      "weighted avg       0.69      0.75      0.69       624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for training and testing data\n",
    "train_predictions = svm_classifier.predict(X_train_tfidf)\n",
    "test_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy = accuracy_score(train_data['labels'], train_predictions)\n",
    "test_accuracy = accuracy_score(test_data['labels'], test_predictions)\n",
    "train_report = classification_report(train_data['labels'], train_predictions)\n",
    "test_report = classification_report(test_data['labels'], test_predictions)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "print(\"\\nTraining Classification Report:\\n\", train_report)\n",
    "print(\"\\nTesting Classification Report:\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_classifier_glove = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train the classifier using the training data (GloVe representations)\n",
    "svm_classifier_glove.fit(X_train_glove, train_data['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (GloVe): 0.7808493589743589\n",
      "Testing Accuracy (GloVe): 0.7548076923076923\n",
      "\n",
      "Training Classification Report (GloVe):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.97      0.86      1745\n",
      "     neutral       0.00      0.00      0.00       151\n",
      "    positive       0.79      0.43      0.56       600\n",
      "\n",
      "    accuracy                           0.78      2496\n",
      "   macro avg       0.52      0.47      0.47      2496\n",
      "weighted avg       0.73      0.78      0.74      2496\n",
      "\n",
      "\n",
      "Testing Classification Report (GloVe):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.95      0.85       436\n",
      "     neutral       0.00      0.00      0.00        38\n",
      "    positive       0.69      0.39      0.50       150\n",
      "\n",
      "    accuracy                           0.75       624\n",
      "   macro avg       0.49      0.44      0.45       624\n",
      "weighted avg       0.70      0.75      0.71       624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ekyus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for training and testing data\n",
    "train_predictions_glove = svm_classifier_glove.predict(X_train_glove)\n",
    "test_predictions_glove = svm_classifier_glove.predict(X_test_glove)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_glove = accuracy_score(train_data['labels'], train_predictions_glove)\n",
    "test_accuracy_glove = accuracy_score(test_data['labels'], test_predictions_glove)\n",
    "train_report_glove = classification_report(train_data['labels'], train_predictions_glove)\n",
    "test_report_glove = classification_report(test_data['labels'], test_predictions_glove)\n",
    "\n",
    "print(\"Training Accuracy (GloVe):\", train_accuracy_glove)\n",
    "print(\"Testing Accuracy (GloVe):\", test_accuracy_glove)\n",
    "print(\"\\nTraining Classification Report (GloVe):\\n\", train_report_glove)\n",
    "print(\"\\nTesting Classification Report (GloVe):\\n\", test_report_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize another SVM classifier for BERT\n",
    "svm_classifier_bert = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train the classifier using the training data (BERT representations)\n",
    "svm_classifier_bert.fit(X_train_bert, train_data['labels'])  # Note: Using train_data since we took a subset for BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (BERT): 0.9198717948717948\n",
      "Testing Accuracy (BERT): 0.6794871794871795\n",
      "\n",
      "Training Classification Report (BERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.95      1745\n",
      "     neutral       0.99      0.80      0.89       151\n",
      "    positive       0.93      0.77      0.84       600\n",
      "\n",
      "    accuracy                           0.92      2496\n",
      "   macro avg       0.95      0.85      0.89      2496\n",
      "weighted avg       0.92      0.92      0.92      2496\n",
      "\n",
      "\n",
      "Testing Classification Report (BERT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.84      0.81       436\n",
      "     neutral       0.13      0.11      0.12        38\n",
      "    positive       0.44      0.36      0.40       150\n",
      "\n",
      "    accuracy                           0.68       624\n",
      "   macro avg       0.45      0.43      0.44       624\n",
      "weighted avg       0.66      0.68      0.67       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiments for training and testing data\n",
    "train_predictions_bert = svm_classifier_bert.predict(X_train_bert)\n",
    "test_predictions_bert = svm_classifier_bert.predict(X_test_bert)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "train_accuracy_bert = accuracy_score(train_data['labels'], train_predictions_bert)\n",
    "test_accuracy_bert = accuracy_score(test_data['labels'], test_predictions_bert)\n",
    "train_report_bert = classification_report(train_data['labels'], train_predictions_bert)\n",
    "test_report_bert = classification_report(test_data['labels'], test_predictions_bert)\n",
    "\n",
    "print(\"Training Accuracy (BERT):\", train_accuracy_bert)\n",
    "print(\"Testing Accuracy (BERT):\", test_accuracy_bert)\n",
    "print(\"\\nTraining Classification Report (BERT):\\n\", train_report_bert)\n",
    "print(\"\\nTesting Classification Report (BERT):\\n\", test_report_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
